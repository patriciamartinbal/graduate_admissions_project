{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROYECTO OPEN DATA II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTACIÓN DEL ALGORÍTMO DE REGRESIÓN LINEAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la segunda parte de esta asignatura nos hemos sumergido en el mundo de programación con pyspark, utilizado en la dinámica de programación en distribuido. A través de pyspark hemos trabajado con un algoritmo de machine learning, asi pues pudiendo trabajar con las bases de uno de los fenómenos más populares en el mercado ahora mismo.\n",
    "\n",
    "El problema que presenta nuestro dataset, Graduate Admissions, obtenido de la página web kaggle.com, es predecir la posibilidad que tiene un alumno específico, basándose en sus calificaciones y otras carácterísticas que listaremos más adelante, en ser admitido para cursar un máster en una universidad en particular. Para ello, se nos proporciona al inicio una variedad de parámetros que son considerados importantes a la hora de realizar la apliación para los programas de máster. \n",
    "Los parámetros son los siguientes:\n",
    "\n",
    "1. GRE Scores ( de 0 a 340 )\n",
    "2. TOEFL Scores ( de 0 a120 )\n",
    "3. Valoración de la universidad (de 0 a 5 )\n",
    "4. Declaración de propósito y carta de recomendación (de 0 a 5)\n",
    "5. GPA Scores (de 0 a 10)\n",
    "6. Experiencia en investigación (0 o 1)\n",
    "7. Probabilidad de ser admitido (entre 0 y 1)\n",
    "\n",
    "Para poder realizar el objetivo del dataset, es decir, predecir la posibilidad de poder entrar, hemos optado por la implementación de un algoritmo de regresión lineal. \n",
    "La elección de este en concreto se basa en que su propósito es establecer un modelo para la relación entre características y una variable objetivo, por lo que se ajusta muy correctamente a nuestras necesidades. Para el dataset con el que trabajamos nosotras, la primera consiste de una serie de calificaciones optenidas por cierto estudiante y la segunda será la probabilidad de entrada de este estudiante en un máster en concreto.\n",
    "\n",
    "Aplicamos nuestro algoritmo, y lo utilizamos para calcular algunos valores que nos ayudan a medir la eficacia de este. A continuación manejamos una mejora del modelo en cuestión, empleando técnicas de hyper-tuning de los parámetros y grid search, y acabando con una extracción de características y PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparación del entorno de trabajo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos el entorno de programcación con pyspark, importandonos los módulos necesarios.\n",
    "En este caso trabajamos con SparkContext, SparkSession y SQLContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc=SparkContext(master=\"local[3]\")\n",
    "print(sc)\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación nos descargamos el dataset y realizamos la limpieza necesaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"com.databricks.spark.csv\").options(header='true',inferschema='true').load(\"Admission_Predict.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo hemos considerado necesario cambiar el nombre de una columna, ya que tenía un punto que en algunos métodos daba problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"Serial No.\", \"Serial No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alogritmo de Regresión Lineal**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, convertimos los datos a dense vector: creamos features y label, las etiquetas con las que suelen trabajar los algoritmos machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed= transData(df)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \\\n",
    "                               outputCol=\"indexedFeatures\").fit(transformed)\n",
    "data = featureIndexer.transform(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(5,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos los datos en datos de entrenamiento (training) y de testeo (test) (60% para training y 40% para testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = transformed.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.show(5)\n",
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos algoritmo de Regresion Lineal. Tiene tres parámetros, a los que para empezar ponemos valores por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(maxIter=2, regParam=0.5, elasticNetParam=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos arquitectura Pipeline (segmentación de instrucciones): Permite implementar el paralelismo a nivel de instrucción en un único procesador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[featureIndexer, lr])\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos un resumen de nuestro modelo. Sacamos los resultados de los siguientes:\n",
    "* **Error cuadrático medio**: un estimador que mide el promedio de los errores al cuadrado, es decir, la diferencia entre el estimador y lo que se estima\n",
    "* **RMSE**: mide las diferencias entre los valores predichos por un modelo o un estimador y los valores observados.\n",
    "* **R cuadrado**: (coeficiente de determinación) para predecir futuros resultados/probar una hipótesis (como de bien se pueden predecir futuros resultados). Se puede interpretar con como de cerca están los datos a la linea de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = model.stages[-1]\n",
    "trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelsummary(model):\n",
    "    import numpy as np\n",
    "    Summary=model.summary\n",
    "    \n",
    "    print (\"##\",'-----------------------------------------------------')\n",
    "    print (\"##\",\"Error Cuadrático Medio: % .6f\" \\\n",
    "           % Summary.meanSquaredError, \", RMSE: % .6f\" \\\n",
    "           % Summary.rootMeanSquaredError )\n",
    "    print (\"##\",\"R cuadrado: %f\" % Summary.r2, \", \\\n",
    "    Total iteraciones: %i\"% Summary.totalIterations)\n",
    "    print (\"##\",'-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsummary(model.stages[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a realizar predicciones con el test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"features\",\"label\",\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluación de resultados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\",\n",
    "                                predictionCol=\"prediction\",\n",
    "                                metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Raiz del error cuadrático medio (RMSE) de los datos de prueba = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = predictions.select(\"label\").toPandas()\n",
    "y_pred = predictions.select(\"prediction\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "r2_score = sklearn.metrics.r2_score(y_true, y_pred) \n",
    "print('valor r2_: {0}'.format(r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mejora del modelo: Hyper-tunning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El hyper-tunning trata de encontrar la mejor opción de combinación de valores de los parámetros de entrada de nuestro modelo. De tal forma podemos entrenar nuestro modelo previo para encontrar su rendimiento más óptimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid search** es un algoritmo que itera a través de la lista de valores de los parámetros y estima los modelos de manera independiente y escoje la mejor opción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, especificamos a nuestro modelo la lista de parámetros sobre la que vamos a iterar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune\n",
    "import pyspark.ml.classification as cl\n",
    "import pyspark.ml.evaluation as ev\n",
    "\n",
    "linear = LinearRegression(labelCol='label',featuresCol = 'indexedFeatures')\n",
    "grid = tune.ParamGridBuilder().addGrid(linear.maxIter, [2, 10, 30]).addGrid(linear.regParam, [0.01, 0.07, 0.5]).addGrid(linear.elasticNetParam, [0.5, 0.4, 0.8]).build()\n",
    "\n",
    "# BinaryClassificationEvaluator lo utilizamos para comparar los modelos (a través de la comparación de su rendimiento)\n",
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = tune.CrossValidator(estimator=linear, estimatorParamMaps=grid, evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = Pipeline(stages=[featureIndexer])\n",
    "model2 = pipeline2.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cvModel nos devuelve el mejor modelo estimado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(model2.transform(trainingData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sacamos el area bajo la curva ROC y PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = model2.transform(testData)\n",
    "results = cvModel.transform(data_train)\n",
    "\n",
    "print(evaluator.evaluate(results, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(results, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluación de los mejores parámetros para nuestro modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    (\n",
    "        [\n",
    "            {key.name: paramValue} \n",
    "            for key, paramValue \n",
    "            in zip(\n",
    "                params.keys(), \n",
    "                params.values())\n",
    "        ], metric\n",
    "    ) \n",
    "    for params, metric \n",
    "    in zip(\n",
    "        cvModel.getEstimatorParamMaps(), \n",
    "        cvModel.avgMetrics\n",
    "    )\n",
    "]\n",
    "\n",
    "sorted(results, key=lambda el: el[1], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Param (MaxIter): ', cvModel.bestModel._java_obj.getMaxIter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Param (RegParam): ', cvModel.bestModel._java_obj.getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best Param (ElasticNetParam): ', cvModel.bestModel._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez obtenidos los mejores parámetros para nuestro modelo, lo entrenamos con estos parámetros para obtener la mejor predicción en base a nuestros datos de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, queremos remarcar el conocimiento obtenido en cuanto a los sistemas distribuídos, tanto su funcionalidad como la cantidad de diferentes opciones que existen hoy en día para poder hacer uso de ellos, ya que permiten utilizar grandes cantidades de datos sin tener que contar físicamente con un equipo extremadamente moderno y preparado para soportar estas grandes cantidades de datos. Ya que, aunque en nuestro caso específico, no teníamos muchos problemas con el tamaño del dataset, sabemos que en el futuro trabajaremos con el doble y el triple de datos, por lo que nos será de gran ayuda haber conocido de los sistemas distribuídos. \n",
    "\n",
    "En segundo lugar, nos gustaría mencionar que haber podido trabajar con pyspark nos abre un nuevo campo de trabajo aún por desarrollar, pero que tiene muchas ventajas en comparación con python que es con lo que trabajamos el anterior curso. Una ventaja que hemos percibido es que es muy veloz a la hora de computacionar, ya que con la cantidad de datos que tenemos, se obtiene una rápida respuesta, y se podría haber hecho mucho más complicado trabajar con un gran volumen de datos de la otra manera. Además, permite usar librerías como MLLib que proporcionan una gran variedad de algoritmos machine learning como regresión logítica, lineal, random forest, etc. Estos algoritmos son perfectos para ampliar nuestro conocimento en el tema y verlo de manera práctica. Aunque tiene como desventaja que es un sistema muy nuevo y existe muy poca documentación sobre él, sabemos que en un futuro cercano esto cambiará y las ventajas serán totales.\n",
    "\n",
    "Por otro lado, el conocimiento sobre los algoritmos de machine learning es otro tema que nos ha marcado durante el transcurso de la asignatura, ya que es un tema nuevo para nosotras con el que nunca habíamos trabajado y el aprender, tanto de manera teórica con las presentaciones primeras como de manera práctica en nuestro proyecto, nos ha enriquecido mucho. Además, gracias a estos, hemos aprendido a mejorar los modelos de manera que el valor que aportan los datos, tanto a nosotras de manera educacional como a las empresas para obtener beneficio de estos, sea mayor y tenga una mejor precisión. \n",
    "\n",
    "En cuanto a los resultados que hemos obtenido para la predicción de entrada a una universidad, hemos de remarcar que, la aplicación de las mejoras ha hecho que redujésemos el error cuadrático medio, el cual cuanto menor fuese, mejor sería la predicción. Obtuvimos sin aplicar ninguna mejora un ECM de 0.020416. Tras aplicar hyper-tunning y obteniendo los mejores parámetros para la regresión lineal un ECM de 0.003526. Por último, aplicando Chi-Cuadrado y PCA un ECM de 0.003518. No hay apenas diferencia en los últimos datos, pero pensamos que esto es porque no tenemos una gran cantidad de parámetros por lo que esto no afectará en gran medida a los cálculos, pero en futuros datasets podría ser de gran ayuda.\n",
    "\n",
    "Por último, creemos que esta asignatura nos ha proporcionado una visión más amplia del mundo profesional en el que vamos a convivir en unos años, ya que pyspark, como he mencionado antes, será uno de los principales métodos para trabajar en el análisis de datos y haber trabajado con esto, nos muestra como podría ser nuestro futuro profesional y nos aporta una gran ventaja competitiva frente a otros que no lo hayan usado.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
